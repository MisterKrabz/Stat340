---
title: "Homework 5"
author: "Your name here"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,eval=T,message=F,warning=F,fig.align='center')
library(tidyverse)
```

## Problem 1. Testing Warm Up: Type 1 and Type 2 Errors <small>(8 pts; 2 pt each)</small>

For the problems below, explain what a Type 1 and Type 2 error would be. 

a. Prospectors are trying to determine if there is evidence that there is a cobalt vein under a mountain.
> type 1: they detect there is evidence of a cobalt vein under a mountain even though there is not
> type 2: they detech there is no evidence of a cobalt vein under a mountain even though there is 

b. The EPA is investigating water quality in a small town, to determine if the lead levels are unsafe.
> type 1: they detect there isevidence of unsafe levels of lead even though the lead levels are safe 
> type 2: they detect there is no evidence of unsafe leevels of lead even though the lead levels are at unsafe levels 

c. A professor suspects a student wrote their essay using ChatGPT, and does some tests.
> type 1: the professor detects the student wrote their essay using chatgpt even though they did not 
> type 2: the profeessor does not detect that the student wrote their essay with chatgpt even though they did 

d. A Machine Learning model is being developed to detect the presence of traffic lights in images.
> type 1: the model detects the presence of traffic lights in images even when there are no traffic lights in the image 
> type 2: the model does not detect there are traffic lights in the image even when there are traffic lights in the image  

## Problem 2. Testing coin flips <small>(10 pts)</small>

In the six sequences below, only one of them is actually **randomly generated from independent flips of a fair coin**. Use a combination of everything you know (common sense, Monte Carlo, hypothesis testing, etc.) to identify which is actually random and explain your reasoning.

(For full points, conduct a formal test and report a $p$-value for each sequence. You may use a combination of multiple tests to arrive at your answer. Any test statistic you employ should be intended to measure some evidence of non-randomness, but should not be specifically tailor-made to a particular sequence If you cannot compute a $p$-value for each sequence, you can still earn a significant amount of partial credit by carefully explaining your reasoning and response as best as you can.)

My advice is **be creative** with the test statistics you come up with to eliminate each sequence! Think of some way of summarizing a sequence of flips that might be useful for comparing against a simulated sequence of random flips. After you come up with an idea for a statistic, remember to run it on many MC generated completely random flips to produce a distribution under the null, which you can then compare with your data to get a p-value. Also, be careful of now you define "more extreme" than the data.

*3 bonus points available if you can find a single test statistic that is powerful enough to reject all the fake sequences (i.e. the $p$-value for five of the sequences are < 0.05). Does such a test statistic exist?*

> for my test statistics, I will use # of rounds resulting in consecutive outcomes, length of longest number of consecutive outcomes, and the triplet deviation (the distribution of triplets of flips, etc: HHT = 1, HHH = 0, ...)

> H0: Sequence is random
> Ha: Sequence is not random 

```{r}
flips1 = "HTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHT"

flips2 = "HHHTHTTTHHTHHTHHHTTTTHTHTHHTTHTHHHTHHTHTTTHTHHHTHTTTHTHTHHTHTHTTHTHHTHTHTTTHTHHHTHTHTTHTHTHHTHTHTHHHTHTTTHTHHTHTHTHHTTTHTHHTHHTTTTHTHTHHHTHTTHTHHTHTHTTHTHHTHTHHHTHHHTHTTTHTTHTTTHTHHHTHTHTTHTHHTHHTHTTT"

flips3 = "HHTHTHTTTHTHHHTHHTTTHTHHTHTTTHTHTHHTHTHTTHTHHHHHHTTTHTHTHHTHTTTHTHHTHTHTTTHTHHHTTHTTTHTHTHHHHTHTTHHTTTTTHTHHHTHTHTTTTTHHHTHHTHHTHHHTTTTHTHTHHHTHHTTTTTHTHHHTHTHTHTTTHTHHHTHTHTHTTHTHHTHTHTHTTTTHTHHHTHTH"

flips4 = "HTHHHHHHHTHTTHHTTHHHTHTHTTTHHTHHHTHHTTHTTTTTTTTTHTHHTTTTTHTHTHTHHTTHTTHTTTTTHHHTHTTTHTHTHHHTHTTTTHTHTHHTTHTHTTHHTHTHHHHTHTTHHTTHTTHTTHTHHHHHHTTTTTTHHHTTHTHHHHTTTHTTHHHTTHTHHTTTHHTHHTTTHTHHTHHHTHHTTHHH"

flips5 = "HHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTTHHTT"

flips6 = "TTHTTTHTTTTTTTHTHTHTHTTHTTHTHHTHHTTTHHTHTTTHTHHTHHHTHTTHHTHHTTHTHTTTTHTHTTTHHTTTTTTTTHTHHTTHTTTTTTHTHTHTHTTTHTTHHTTHTTTHHTTTHTTHTTTTHTTTTHHTTTHTHTHHHTTTTTTHTHHTTTTTTTTTTTTHHHTTTHHHTTTHTTTHTHTTHTTTTTHT"

split = function(str) strsplit(str, split="")[[1]]

s1 = split(flips1)
s2 = split(flips2)
s3 = split(flips3)
s4 = split(flips4)
s5 = split(flips5)
s6 = split(flips6)
sequences = list(s1, s2, s3, s4, s5, s6)
sequence_names = c("flips1", "flips2", "flips3", "flips4", "flips5", "flips6")

count_consecutive = function(flips) {
  if (length(flips) == 0) {
    return(0)
  }
  
  consecutive_count = 1
  
  for (i in 2:length(flips)) {
    if (flips[i] != flips[i-1]) {
      consecutive_count = consecutive_count + 1
    }
  }
  
  return(consecutive_count)
}

longest_consecutive_sequence = function(flips) {
  if (length(flips) == 0) {
    return(0)
  }

  max_length = 1
  current_length = 1

  for (i in 2:length(flips)) {
    if (flips[i] == flips[i-1]) {
      current_length = current_length + 1
    } else {
      max_length = max(max_length, current_length)
      current_length = 1
    }
  }
  return(max(max_length, current_length))
}

calculate_triplet_deviation = function(flips) {
  if (length(flips) <= 2) {
    return(0)
  }
  
  triplets = c()
  for (i in 1:(length(flips) - 2)) {
    triplets = c(triplets, paste0(flips[i], flips[i+1], flips[i+2]))
  }
  
  all_triplets = c("HHH", "HHT", "HTH", "HTT", "THH", "THT", "TTH", "TTT")
  observed_counts = table(factor(triplets, levels = all_triplets))
  
  expected_count = (length(flips) - 2) / 8
  
  deviation = sum((observed_counts - expected_count)^2)
  return(deviation)
}


set.seed(2025)
N_mc = 100000
n_flips = length(s1)

null_consecutive_counts = seq(0, 0, length.out = N_mc)
null_longest_consecutive = seq(0, 0, length.out = N_mc)
null_triplet_deviation = seq(0, 0, length.out = N_mc)

for (i in 1:N_mc) {
  random_flips = sample(c("H", "T"), size = n_flips, replace = TRUE)
  
  null_consecutive_counts[i] = count_consecutive(random_flips)
  null_longest_consecutive[i] = longest_consecutive_sequence(random_flips)
  null_triplet_deviation[i] = calculate_triplet_deviation(random_flips)
}

calculate_p_value = function(observed_stat, null_distribution) {
  mean_null = mean(null_distribution)
  deviation = abs(observed_stat - mean_null)
  
  p_value = mean(abs(null_distribution - mean_null) >= deviation)
  return(p_value)
}

results = data.frame(
  Sequence = sequence_names,
  P_Consecutive = seq(0, 0, length.out = 6),
  P_Longest_Consecutive = seq(0, 0, length.out = 6),
  P_Triplet_Deviation = seq(0, 0, length.out = 6)
)

for (i in 1:length(sequences)) {
  current_flips = sequences[[i]]
  
  obs_c = count_consecutive(current_flips)
  obs_lc = longest_consecutive_sequence(current_flips)
  obs_td = calculate_triplet_deviation(current_flips)
  
  results$P_Consecutive[i] = calculate_p_value(obs_c, null_consecutive_counts)
  results$P_Longest_Consecutive[i] = calculate_p_value(obs_lc, null_longest_consecutive)
  results$P_Triplet_Deviation[i] = calculate_p_value(obs_td, null_triplet_deviation)
}

print(results)
```
> flips4 is the one that is the most likely sequence to be randomly generated because the p values for all categories on flips4 are all in general consistently significantly above average as compared to the other flip sequences. I was able to use triplet deviation to get the extra credit, as with triplet deviation i was able to find that the p-value of flips4 is significantly higher than all the other flips sequences to the point where the p values of all the other flips sequences is less than 0.05 

## Problem 3. Finding the Trick Coin <small>(6 pts; 2pts each)</small>

I have two coins in my pocket - a trick coin with two heads and a fair coin with one head and one tail(s?). We'll play a game. I will grab one coin at random, and flip it $N$ times. After that you will have to decide if it is the fair coin or the trick coin. The null hypothesis is that it is the fair coin. 

**Decision Rule 1**: If after $N$ flips there are no tails, then you decide it is the trick coin. If there is at least 1 tail then you know it is the fair coin. 

a. Using "Decision Rule 1", what is the lowest number of flips $N$ would you need in order to have a significance level less than 5% for this test?

> H0: p(tails) > 0
> Ha: P(tails) = 0

```{r}
num_flips = 1:10 
set.seed(2025)

chart <- data.frame(
  num_flips = num_flips,
  significance_level = numeric(length(num_flips))
)

simulate_flip <- function() {
  runif(1)
}

num_simulations <- 10000

for (n in chart$num_flips) {
  all_heads_count <- 0

  for(i in 1:num_simulations){
    flips <- runif(n)
    
    tails_count <- sum(flips < 0.5)
    
    if (tails_count == 0) {
      all_heads_count <- all_heads_count + 1
    }
  }
  
  estimated_alpha <- all_heads_count / num_simulations
  
  chart$significance_level[n] <- estimated_alpha
}

chart
```
> From our data frame we can see that the first valid number of flips that results in a p value lower than 0.05 occurs at 5 flips that result in all heads. therefore at 5 flips you can be 95% confident that the coin is rigged. 

b. Using $N$ from part a, what is the power of the test?
> the power of the test is 1 because when the rigged coin is being flipped, we will never get tails, so the probability of rejecting H0 when the trick coin is flipped is 100%. 

c. Suppose $N=4$ is decided. How can you modify the decision process to have a significance level of exactly 5%? Does this change the power of the test? (*Hint: There are a few ways to do this; One way is to introduce some randomness into your decision*)
```{r}
0.05/0.0605
```
> Basically, we will need to randomly fail to reject .826 of the coins that end up with combination HHHH while we will need to randomly reject .174 of the coins that end up with combination HHHH. Now, the power of the test has been redued to .826 since we have a higher probability of rejecting H0 when the trick coin is flipped.

## Problem 4. Testing the maximum of a uniform distribution <small>(8 pts; 2 pts each)</small>

We sample $X_1, X_x,\ldots,X_n \overset{\text{iid}}\sim\text{Uniform}(0,m)$ where $m$ is an unknown maximum. Sleazy Jim tells you that $m=1$ but you're not so sure. The 50 values sampled are in the following data file:

```{r}
X <- read.csv("../../data/uniform_sample.csv")$x

X
```

a. Write out in formal notation the null and alternative hypotheses. 
> H0: m = 1
> Ha: m < 1

b. Come up with a test statistic and measure your sampled data. Is this a one-sided test or two-sided test?
```{r}
max(X)
```
> Test statistic: 0.9368
> one sided test 

c. Simulate a distribution for the test statistic under the null hypothesis of size at least 1000. Display a histogram of your test statistic distribution.
```{r}
n_simulations <- 10000
m_null <- 1
n <- 50

simulated_maxes <- c()

for (i in 1:n_simulations) {
  sim_sample <- runif(n = n, min = 0, max = m_null)
  simulated_maxes[i] <- max(sim_sample)
}

plot_data <- data.frame(maxes = simulated_maxes)

ggplot(plot_data, aes(x = maxes)) +
  geom_histogram(
    bins = 50, 
    fill = "black",
    color = "green"
  )

```
d. Calculate the $p$-value for this data and make a conclusion.
```{r}
obs_max
m_null
head(simulated_maxes)
summary(simulated_maxes)
mean(simulated_maxes <= obs_max)
```
> p = 2e-4 < 0.05 = alpha, reject h0. We have statistically significant evidence that the true max of the our population of our values is not equal to 1. 

## Problem 5. Blurtle <small>(10 pts; 2 pt each)</small>

Have you been playing the hot new game Blurtle? It's a (fictional) word game you can play daily - you have to guess a 5 letter word and you only have 6 attempts. I've been playing for the past 100 days and I've been tracking my number of guesses. I'm trying to figure out whether I have been getting better or not.

The file `blurtle.csv` contains 100 rows of data, giving the number of tries to guess the word. If it took 7 guesses that actually means I failed (you don't actually get a 7th guess). 

```{r}
blurtle <- read.csv("../../data/blurtle.csv")

blurtle
```

Your task is to perform a permutation test on the data to determine if there is statistical evidence of a true improvement trend.

a. State the null and alternative hypotheses
> H0: average number of tries before day 50 = average number of tries after day 50 
> Ha: average number of tries before day 50 < average number of tries after day 50 

b. Determine a test statistic that identifies a trend in the number of tries. There are many good ones you could use - be creative.
```{r}
mean(blurtle$tries[1:50]) - mean(blurtle$tries[51:100])
```
> the difference in the mean number of tries between the first 50 and last 50 days: 0.22 

c. Decide whether the test will be a one or two-tailed test
> One tailed test because we are trying to see whether it takes less tries after day 50 rather than trying to see if there is a difference 

d. Simulate a distribution of test statistics under the null hypothesis
```{r}
perm_diffs <- c()

for (i in 1:n_permutations) {
  shuffled_tries <- sample(blurtle$tries)
  
  perm_first_half <- shuffled_tries[1:50]
  perm_second_half <- shuffled_tries[51:100]
  
  perm_diffs[i] <- mean(perm_first_half) - mean(perm_second_half)
}
mean(perm_diffs)
```

e. Calculate the test statistic on the observed data, calculate the $p$-value and state your conclusions.
```{r}
set.seed(2025)

perm_diffs <- c()

observed_diff <- mean(blurtle$tries[1:50]) - mean(blurtle$tries[51:100])
n_permutations <- 100000

for(i in 1:n_permutations){
  # shuffle the entire burdle dataset 
  shuffled_tries <- sample(blurtle$tries)
  
  # split them into half and half and find the mean difference 
  perm_first_half <- shuffled_tries[1:50]
  perm_second_half <- shuffled_tries[51:100]
  perm_diffs[i] <- mean(perm_first_half) - mean(perm_second_half)
}

p_value <- sum(perm_diffs >= observed_diff) / n_permutations
p_value
```
> p value = .09616 > 0.05 = alpha, fail to reject H0. We do not have statistically significant evidence that there was an improvement in baffle results after 50 days of practicing. 

# Problem 6. What Wheat <small>(8 pts; 2 pt each)</small>

Suppose we have two varieties of wheat: standard and enhanced. The standard variety produces about 16-30 plants per square foot while the enhanced variety can produce between 18-35. Suppose the number of plants is approximately normally distributed (we could round the continuous value to the nearest integer, but you can keep the simulated values as decimals for this problem).

$X_{standard} \sim N(23, 2.4^2)$

$X_{enh} \sim N(26.5, 2.8^2)$.

Suppose you have a bin of wheat for planting but you don't know which variety it is. You plant 5 square feet of wheat and count the plants that grow.

The null hypothesis is that the variety is standard.

a. If we use the number of plants that grow as our test statistic, would you consider this a one-sided or two-sided test? 
> one sided. Because the null hypothesis says that the variety is standard and the only other variety that the wheat could be is enhanced, then the only test that we can run is whether or not the batch grows more than what our standard wheat does. 

b. Simulate the experiment 1000 times under the null hypothesis and plot the distribution of simulated test statistics.
```{r}
set.seed(2025)

n_sims <- 1000
sample_size <- 5
significance_level <- 0.05

mu_null <- 23
sd_null <- 2.4

mu_alt <- 26.5
sd_alt <- 2.8

null_dist_means <- numeric(n_sims)

for (i in 1:n_sims) {
  sample_data <- rnorm(n = sample_size, mean = mu_null, sd = sd_null)
  
  null_dist_means[i] <- mean(sample_data)
}

plot_df <- data.frame(means = null_dist_means)

wheat_plot <- ggplot(plot_df, aes(x = means)) +
  geom_histogram(bins = 20, fill = "black", color = "green") 

print(wheat_plot)
```

c. If your significance level is 0.05, what rejection threshold, or rejection regions, would you use when classifying the wheat as "enhanced."
```{r}
rejection_threshold <- quantile(null_dist_means, 1 - significance_level)
rejection_threshold
```
> 24.7209

d. Using the threshold from part c, how likely are you to correctly classify "enhanced" wheat?
```{r}
alt_dist_means <- c()

for (i in 1:n_sims) {
  sample_data <- rnorm(n = sample_size, mean = mu_alt, sd = sd_alt)
  
  alt_dist_means[i] <- mean(sample_data)
}

correct_classifications <- sum(alt_dist_means > rejection_threshold)
correct_classifications / n_sims
```
> 92.2% 
