---
title: "Homework 6 - Solutions"
author: "Patrick Wang"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,eval=T,message=F,warning=F,fig.align='center')
library(tidyverse)
```

## Problem 1. More Widgets <small>(8 pts; 2pts each)</small>

Let's return to the example from lecture - Universal Widgets of Madison. Suppose that the probability of a successful widget, $p$, is 0.93 (as opposed to the 0.80 from lecture). We will assume independence between each widget. 

a. If we are sampling 200 widgets, use Monte Carlo to estimate the probability of getting between 90% and 95% functional in your sample.
```{r}
n_simulations <- 100000
sample_size <- 200
p_functional <- 0.93

set.seed(2025)

num_functional <- rbinom(n_simulations, size = sample_size, prob = p_functional)

sample_proportions <- num_functional / sample_size

mean(sample_proportions >= 0.90 & sample_proportions <= 0.95)
```
> 0.85591 

b. What is the variance of $\bar{X}$? Answer this based on the theoretical variance.
```{r}
.93*(1-.93)/200
```
> 0.0003255

c. Use Monte Carlo to confirm your answer from part b is correct.
```{r}
set.seed(2025)

num_functional <- rbinom(n = n_simulations, size = sample_size, prob = p_functional)
sample_proportions <- num_functional / sample_size
var(sample_proportions)
```
> 0.0003265

d. If the true $p$ is 0.93, and you sample 200 widgets, what are the 2.5th and 97.5th percentiles of the sampling distribution of $\bar{X}$? 
```{r}
quantile(sample_proportions, prob = 0.025)
quantile(sample_proportions, prob = 0.975)
```
> 2.5%: 0.895
> 97.5%: 0.965

## Problem 2. Estimating Quantiles <small>(8 pts; 2pts each)</small>

There are 9 algorithms in R to estimate population quantiles. Type `?quantile` to read about them. Here we will investigate the bias and variance of some of these estimators. To use the quantile function you use the syntax
`quantile(vector, probs, type)`. For example if you have data in a vector called `sampleData` and you wish to estimate the 80th percentile using algorithm 7 (the default), you use `quantile(sampleData, .80, type=7)`

Suppose we're interested in the 80th percentile for $X$, and we know that $X$ follows a uniform distribution but we don't know the parameters (if we did we wouldn't have to estimate). We want to randomly sample $n=30$ values and estimate the 90th percentile. Using MC simulation estimate the following:

a. Which quantile algorithm (4 through 9) has bias closest to zero (you can use absolute bias $|E\hat{\theta}-\theta| \approx |mean(\hat{\theta})-\theta|$)? *Hint: you can use $unif(0,1)$ for the purposes of this estimation, as your answer won't depend on the upper and lower bounds chosen.*
```{r}
n_simulations <- 100000
sample_size <- 30
true_quantile <- 0.9
quantile_types <- 4:9

estimates <- sapply(quantile_types, function(type) {
  replicate(n_simulations, {
    sample_data <- runif(sample_size, 0, 1)
    quantile(sample_data, probs = 0.9, type = type)
  })
})

colnames(estimates) <- c("Type_4", "Type_5", "Type_6", "Type_7", "Type_8", "Type_9")

biases <- colMeans(estimates) - true_quantile
abs(biases)
```
> algorithm 6 

b. Which quantile algorithm (4 through 9) has the smallest variance?
```{r}
var(estimates[,1])
var(estimates[,2])
var(estimates[,3])
var(estimates[,4])
var(estimates[,5])
var(estimates[,6])
```
> algorithm 6

c. Which method is best for estimating the 90th percentile from a uniform distribution? Justify your answer from parts a and b.
```{r}
var(estimates[,1]) + biases[1]^2
var(estimates[,2]) + biases[2]^2
var(estimates[,3]) + biases[3]^2
var(estimates[,4]) + biases[4]^2
var(estimates[,5]) + biases[5]^2
var(estimates[,6]) + biases[6]^2
```
> method 6. The best metric is the one with the lowest squared means error (which is method 6 = 0.002732657), since mean squared error combines the bias and variance to give an overall picture of the estimator's quality. 

d. What about if $X\sim N(\mu, \sigma^2)$? Would you prefer a different method for estimating the 80th percentile from a normal distribution? *Hint: repeat the same analysis for $N(0,1)$.*
```{r}
true_quantile_norm <- qnorm(0.9, mean = 0, sd = 1) 

estimates_norm <- sapply(quantile_types, function(type) {
  replicate(n_simulations, {
    sample_data <- rnorm(sample_size, mean = 0, sd = 1) 
    quantile(sample_data, probs = 0.9, type = type)
  })
})

biases_norm <- colMeans(estimates_norm) - true_quantile_norm
abs(biases_norm)

apply(estimates_norm, 2, var)

mse_norm <- apply(estimates_norm, 2, var) + biases_norm^2
mse_norm
```
> yes for a normal distribution the best algorithm changes. it is now algorithm 5 since it has the lowest mean squared error. 

## Problem 3. Estimating a Geometric $p$ <small>(9 pts; 3 pts each)</small>

a. Use the method of moments to come up with an estimator for a geometric distributions parameter $p$. *Hint: Use the fact that if $X\sim Geom(p)$ then $EX=\frac{1-p}{p}*$. 
> Xbar = (1-p)/p
> Xbar * p = 1-p
> Xbarp + p = 1
> p(Xbar + 1) = 1
> phat = 1/(Xbar + 1)

b. Estimate the sampling distribution of this estimator when we sample $n=20$ values from from $Geom(.15)$. Show the histogram of the estimated sampling distribution.
```{r}
n_simulations <- 100000
sample_size <- 20
true_p <- 0.15

set.seed(2025)

p_estimates <- replicate(n_simulations, {
  sample_data <- rgeom(n = sample_size, prob = true_p)
  
  sample_mean <- mean(sample_data)
  
  p_hat <- 1 / (sample_mean + 1)

  return(p_hat)
})

hist(p_estimates, 
     main = "Estimated Sampling Distribution of p-hat", 
     xlab = "Estimated p (p-hat)",
     col = "green",
     border = "black",
     freq = FALSE) 

abline(v = true_p, col = "red", lwd = 2)
legend("topright", legend = paste("True p =", true_p), col = "red", lty = 1, lwd = 2)
```
> the sampling distribution of phat is centered near the true value of p = 0.15. It is unimodal, right skewed. 

c. Estimate the bias of this estimator. Is it biased? Estimate the variance of the estimator.
```{r}
mean(p_estimates) - true_p

var(p_estimates)
```
> since there is an esimated bias of .00672, there is a small positive bias. The variance is .001128825

## Problem 4. Estimating $\lambda$ from a Poisson Distribution<small>(8 pts; 2 pts each)</small>

It is interesting that if $X\sim Pois(\lambda)$ that $EX=VarX=\lambda$. One could use either $\bar{X}$ or $S^2$ as an estimator of $\lambda$ perhaps. 

a. Using $n=15$ and $\lambda=20$ for this problem, use MC simulation to estimate the sampling distribution of The estimator $\bar{X}$. Show its histogram. 
```{r}
n_simulations <- 100000
sample_size <- 15
true_lambda <- 20

set.seed(2025) 

x_bar_estimates <- replicate(n_simulations, {
  sample_data <- rpois(n = sample_size, lambda = true_lambda)
  mean(sample_data)
})

hist(x_bar_estimates,
     xlab = "Estimated lambda using Sample Mean (Xbar)",
     col = "green",
     border = "black",
     freq = FALSE)
abline(v = true_lambda, col = "red", lwd = 2)
legend("topright", legend = "True lambda = 20", col = "red", lty = 1, lwd = 2)
```

b. Repeat the same but this time use $S^2$. 
```{r}
s_squared_estimates <- replicate(n_simulations, {
  sample_data <- rpois(n = sample_size, lambda = true_lambda)
  var(sample_data)
})

hist(s_squared_estimates,
     xlab = "Estimated lambda using Sample Variance (S^2)",
     col = "green",
     border = "black",
     freq = FALSE)
abline(v = true_lambda, col = "red", lwd = 2)
legend("topright", legend = "True lambda = 20", col = "red", lty = 1, lwd = 2)
```

c. Compare the two estimators. Would you prefer one over the other? Why?
```{r}
mean(x_bar_estimates) - true_lambda

var(x_bar_estimates)

mean(s_squared_estimates) - true_lambda

var(s_squared_estimates)

```
> prefer to use Xbar because although both have a very low bias, the variance of xbar is much smaller than for S^2, and an estimator with a lower variance is more reliable. 

d. What about a linear combination of the two variables? Could you construct an estimator of $\lambda$ of the form $a\bar{X} + bS^2$ that would be better than using either of them by themselves? Use the variance of the estimators and their covariance as justification.

> V(a) = Var(aXbar + (1-a)S^2)
> dV/da = 2aVar(Xbar) - 2(1-a)*Var(S^2) + (2-4a)Cov(Xbar, S^2) = 0
> a = (Var(S^2) - Cov(Xbar, S^2))/(Var(Xbar) + Var(S^2) - 2Cov(Xbar, s^2))

```{r}

n_simulations <- 100000
sample_size <- 15
true_lambda <- 20
set.seed(2025)

estimates <- replicate(n_simulations, {
  sample_data <- rpois(n = sample_size, lambda = true_lambda)
  c(mean = mean(sample_data), var = var(sample_data))
})

estimates <- t(estimates)

var_xbar <- var(estimates[, "mean"])
var_s2 <- var(estimates[, "var"])
cov_xbar_s2 <- cov(estimates[, "mean"], estimates[, "var"])

a_optimal <- (var_s2 - cov_xbar_s2) / (var_xbar + var_s2 - 2 * cov_xbar_s2)

a_optimal
```
> since a is basically 1, the best linear combination is probably still just Xbar. A better estimator can be constructed, however the difference in improvement would be extremely small. 

## Problem 5. The Standard Error of $\bar{X}$<small>(8 pts; 2 pts each)</small>

What would be the required sample size $n$ so that the standard error of $\bar{X}$ (i.e. $SD(\bar{X})$) would be 2 (or just under 2) for the following populations:

a. $Normal(1000, 10^2)$
```{r}
(10 / 2)^2
```
> 25

b. $Pois(75)$
```{r}
(sqrt(75) / 2)^2
```
> 19 

c. $Binom(200, .35)$
```{r}
(sqrt(45.5)/2)^2
```
> 12

d. $exp(.05)$
```{r}
(20 / 2)^2
```
> 100

## Problem 6. More Estimators for $\mu$  <small>(9 pts)</small>

Consider a population that is slightly skewed. The function below will generate data from this population. Also you can view a histogram of the population. The true mean of this skewed population is exactly 40.

```{r}
rskewed <- function(n){
  return (rnorm(n, mean=6, sd=2)^2)
}

hist(rskewed(10000), breaks=50)
```

Here are a few estimators for the population mean:

1. $\hat{\mu}_1=\bar{X}$, the sample mean
2. $\hat{\mu}_2$ is the 10% trimmed mean; we remove the highest 5% of values and lowest 5% of values and then calculate the sample mean
3. $\hat{\mu}_3$ is the midrange, the average of the minimum and the maximum.
4. $\hat{\mu}_4$ is the Windsorized mean; like the 10% trimmed mean, but the to 5% biggest values are replaced with the next smallest value, the 5% smallest values are replaced with the next largest value. 

The following function will perform the estimations. To use it you just have to specify which type you want (1 through 4):

```{r}
meanEstimate <- function(x, type=1){
  if(type==1){
    return(mean(x))
  } else if(type==2){
    trimmedX <- x[x<quantile(x, .95) & x>quantile(x, .05)]
    return(mean(trimmedX))
  } else if(type==3){
    return((min(x)+max(x))/2)
  } else if(type==4){
    trimmedX <- x[x<quantile(x, .95) & x>quantile(x, .05)]
    augmentedX <- c(trimmedX, 
                    rep(min(trimmedX),sum(x<=quantile(x, .05))),
                    rep(max(trimmedX),sum(x>=quantile(x, .95))))
    return(mean(augmentedX))
  }
} 
```

Use Monte Carlo to estimate the bias and variance of each of these estimators. Base your estimations on a sample of size 25 from the skewed population.

```{r}
set.seed(2025)
N <- 10000
n <- 25
true_mu <- 40

bias <- var <- rep(NA, 4)

for (t in 1:4) {
  est <- replicate(N, meanEstimate(rskewed(n), type=t))
  bias[t] <- mean(est) - true_mu
  var[t] <- var(est)
}

MSE <- var + bias^2

results <- data.frame(
  Estimator = 1:4,
  Bias = round(bias, 3),
  Variance = round(var, 3),
  MSE = round(MSE, 3)
)

results
```
Which of these 4 estimators would you prefer when estimating the population mean for this skewed population? Justify your choice.

> based on the monte carlo results, the sample mean is the best estimator for the population mean. Its hard to determine from the estimator, bias, and variance data by eyeballing it, but when we use the Mean Squared error, we can see that estimator 1 has the smallest MSE and therefore the it provides the most accurate estimation of the true mean. 

